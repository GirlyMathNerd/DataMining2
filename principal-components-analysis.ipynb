{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Bobby Lindsey, Adam Swayze, Wesley Pryor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries needed for this tutorial\n",
    "from IPython.core.display import display\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA for short) is a technique used to reduce the dimensions of a data set. There are a few helpful ways to explain what PCA does:\n",
    "* PCA computes the most meaningful basis to re-express a noisy, garbled data set.\n",
    "* PCA yields a feature subspace that maximizes the variance along the axes.\n",
    "* PCA reduces the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors (which all have the same unit length of 1) will form the axes.\n",
    "\n",
    "PCA is an extremely useful technique in data mining. Hi-resolution images can have thousands of dimensions (MRI scans are huge) and those familiar with \"the curse of dimensionality\" know that certain algorithms are only approachable when one's data set is of a manageable size. PCA employs a combination of topics like correlation, eigenvalues, and eigenvectors to determine which variables account for most of the variability in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a transformation $\\mathrm{T} =\n",
    "\\begin{bmatrix}\n",
    "7 & -1\\\\\n",
    "-1 & 7\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "To find the eigenvalues, $\\lambda$s, of the transformation:\n",
    "* $\\mathrm{T}x = \\lambda x$\n",
    "* $\\mathrm{T}x = \\lambda \\mathrm{I} x$\n",
    "* $\\mathrm{T}x - \\lambda \\mathrm{I} x = 0$\n",
    "* $(\\mathrm{T} - \\lambda \\mathrm{I})x = 0 : x \\ne 0$ ```part of the definition of an eigenvector```\n",
    "* $x \\in \\mathrm{ker}(\\mathrm{T} - \\lambda \\mathrm{I})$ ```the kernel has a solution iff its solution isn't the trivial one; we can express this with a determinant```\n",
    "* $\\mathrm{det}(\\mathrm{T} - \\lambda \\mathrm{I}) = 0$ ```this is called the characteristic equation```\n",
    "* $\\mathrm{det}(\\lambda \\mathrm{I} - \\mathrm{T}) = 0$\n",
    "* $\\mathrm{det}\\big(\\lambda \\big(\\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}\\big) - \\begin{bmatrix} 7 & -1\\\\ -1 & 7 \\end{bmatrix}\\big) = 0$\n",
    "* $\\mathrm{det}\\big(\\begin{bmatrix} \\lambda & 0\\\\ 0 & \\lambda \\end{bmatrix} - \\begin{bmatrix} 7 & -1\\\\ -1 & 7 \\end{bmatrix}\\big) = 0$\n",
    "* $\\mathrm{det}\\big(\\begin{bmatrix} \\lambda-7 & 1\\\\ 1 & \\lambda-7 \\end{bmatrix}\\big) = 0$\n",
    "* $(\\lambda - 7)^2 - 1 = 0$\n",
    "* $\\lambda^2 - 14 \\lambda + 49 - 1 = 0$\n",
    "* $\\lambda^2 - 14 \\lambda + 48 = 0$\n",
    "* $(\\lambda - 6)(\\lambda - 8) = 0$\n",
    "* $\\lambda = 6, \\lambda = 8$\n",
    "\n",
    "To find the eigenvectors associated with the eigenvalues, we need only plug each eigenvalue into the characteristic equation and solve for the vector or vectors that provide the solution to the equation:\n",
    "* $\\lambda = 6$\n",
    "* $\\mathrm{det}(6 \\mathrm{I} - \\mathrm{T}) = 0$\n",
    "* $\\begin{bmatrix} -1 & 1\\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} a\\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "* rref(above) and solve for pivot variables to get the following set of vectors as a solution - $\\big\\{\\begin{bmatrix} a \\\\ a \\end{bmatrix} : a \\in \\mathbb{R}\\big\\} = \\big\\{a\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} : a \\in \\mathbb{R}\\big\\} = \\mathrm{span}\\big(\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\big)$\n",
    "* We can repeat the above steps with $\\lambda = 8$ and get the set of vectors - $\\mathrm{span}\\big(\\begin{bmatrix} 1 \\\\ -1\\end{bmatrix}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues: [ 8.  6.]\n",
      "\n",
      "normalized eigenvectors:\n",
      " [[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "\n",
      "un-normalized eigenvectors:\n",
      " [[ 1.  1.]\n",
      " [-1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# create a numpy matrix for our transformation\n",
    "T = np.array([[7, -1], [-1, 7]])\n",
    "# get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = LA.eig(T)\n",
    "print(\"eigenvalues: %s\\n\" %eigenvalues)\n",
    "# eigenvectors will be normalized\n",
    "print(\"normalized eigenvectors:\\n %s\\n\" %eigenvectors)\n",
    "#print(eigenvectors)\n",
    "# un-normalized eigenvectors\n",
    "normalized_eigenvectors = eigenvectors*math.sqrt(2)\n",
    "print(\"un-normalized eigenvectors:\\n %s\" %normalized_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA the Hard Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalize data set (always a good idea to do so that certain algorithms aren't affected by the scale of different variables)\n",
    "2. Calculate the correlation matrix for your dataset. This matrix will be your transformation and will provide the correlation between each pair of variables. (We use correlation instead of covariance since correlation is a normalized version of the covariance matrix; although we're not too concerned about normalization since we already normalized the data set in step 1).\n",
    "3. Find the eigenvectors and eigenvalues of that matrix using the steps above.\n",
    "4. For each eigenvalue, take the absolute value and divide it by the sum of all the eigenvalues which will provide the proportion of variance that its associated eigenvector contributes to the data.\n",
    "5. Sort the principal components by their associated eigenvalues (highest to lowest).\n",
    "6. Eigenvalues tell you which principal components to keep. The number of principal components to keep will be based on the cumulative explained variance that the eigenvalues account for. The cumulative explained variance to stop at is a threshold that is decided by how much variability you want to explain.\n",
    "7. Stick each eigenvector you want to keep in a matrix (this is called a \"feature vector\"). We call each eigenvector a \"principal component\".\n",
    "8. Now project the data set onto the new feature space (i.e. the feature vector) by multiplying your data set by the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "[[ 1.         -0.10936925  0.87175416  0.81795363]\n",
      " [-0.10936925  1.         -0.4205161  -0.35654409]\n",
      " [ 0.87175416 -0.4205161   1.          0.9627571 ]\n",
      " [ 0.81795363 -0.35654409  0.9627571   1.        ]]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
      " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
      " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
      " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
      "\n",
      "Eigenvalues in descending order:\n",
      "2.91081808375\n",
      "0.921220930707\n",
      "0.147353278305\n",
      "0.0206077072356\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "print(\"Original data set:\")\n",
    "display(df.tail())\n",
    "# normalize data set\n",
    "data_set = df.ix[:,0:3].values\n",
    "data_set = StandardScaler().fit_transform(data_set)\n",
    "# calculate correlation matrix\n",
    "correlation_matrix = np.corrcoef(data_set.T)\n",
    "print(\"Correlation matrix:\\n%s\\n\" %correlation_matrix)\n",
    "# find eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = LA.eig(correlation_matrix)\n",
    "print(\"Eigenvectors:\\n%s\\n\" %eigenvectors)\n",
    "# make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n",
    "# sort the (eigenvalue, eigenvector) tuples from highest to lowest\n",
    "eigen_pairs.sort()\n",
    "eigen_pairs.reverse()\n",
    "print('Eigenvalues in descending order:')\n",
    "for pair in eigen_pairs:\n",
    "    print(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance:\n",
      "0.727704520938\n",
      "0.230305232677\n",
      "0.0368383195763\n",
      "0.00515192680891\n"
     ]
    }
   ],
   "source": [
    "# find explained variance of eigenvalues\n",
    "eigenvalues_sum = sum(eigenvalues)\n",
    "explained_variance = [(eigenvalue / eigenvalues_sum) for eigenvalue in sorted(eigenvalues, reverse=True)]\n",
    "print(\"Explained Variance:\")\n",
    "for i in explained_variance:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the first two eigenvectors account for the majority of the variance in the data set. Let's just use the two eigenvectors associated with those first two eigenvalues, stick them in a matrix, and apply said matrix to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionally-reduced data set (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870522</td>\n",
       "      <td>-0.382822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.558492</td>\n",
       "      <td>0.905314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.520845</td>\n",
       "      <td>-0.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.376391</td>\n",
       "      <td>-1.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.959299</td>\n",
       "      <td>0.022284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "145  1.870522 -0.382822\n",
       "146  1.558492  0.905314\n",
       "147  1.520845 -0.266795\n",
       "148  1.376391 -1.016362\n",
       "149  0.959299  0.022284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# put eigenvectors in matrix\n",
    "feature_vector = np.hstack((eigen_pairs[0][1].reshape(4,1), \n",
    "                            eigen_pairs[1][1].reshape(4,1)))\n",
    "# project data set onto feature_vector\n",
    "pca_data_set = pd.DataFrame(data_set.dot(feature_vector))\n",
    "print(\"Dimensionally-reduced data set (tail):\")\n",
    "display(pca_data_set.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA the Easy Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the mechanics of Principal Components Analysis, we can use convenience functions in sklearn to do all the heavy lifing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix:\n",
      "[[ 1.         -0.10936925  0.87175416  0.81795363]\n",
      " [-0.10936925  1.         -0.4205161  -0.35654409]\n",
      " [ 0.87175416 -0.4205161   1.          0.9627571 ]\n",
      " [ 0.81795363 -0.35654409  0.9627571   1.        ]]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.52237162  0.37231836 -0.72101681 -0.26199559]\n",
      " [-0.26335492  0.92555649  0.24203288  0.12413481]\n",
      " [ 0.58125401  0.02109478  0.14089226  0.80115427]\n",
      " [ 0.56561105  0.06541577  0.6338014  -0.52354627]]\n",
      "\n",
      "Explained Variance:\n",
      "[ 0.72770452  0.23030523  0.03683832  0.00515193]\n",
      "\n",
      "Dimensionally-reduced data set (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870522</td>\n",
       "      <td>0.382822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.558492</td>\n",
       "      <td>-0.905314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.520845</td>\n",
       "      <td>0.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.376391</td>\n",
       "      <td>1.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.959299</td>\n",
       "      <td>-0.022284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "145  1.870522  0.382822\n",
       "146  1.558492 -0.905314\n",
       "147  1.520845  0.266795\n",
       "148  1.376391  1.016362\n",
       "149  0.959299 -0.022284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scikit-learn convenience command\n",
    "# standardize data set (sklearn uses covariance matrix instead of correlation matrix)\n",
    "data_set = df.ix[:,0:3].values\n",
    "data_set_std = StandardScaler().fit_transform(data_set)\n",
    "sklearn_pca = sklearnPCA(n_components=4)\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# covariance matrix\n",
    "covariance_matrix = sklearn_pca.get_covariance()\n",
    "print(\"Covariance matrix:\\n%s\\n\" %covariance_matrix)\n",
    "# eigenvectors\n",
    "eigenvectors = sklearn_pca.components_.T\n",
    "print(\"Eigenvectors:\\n%s\\n\" %eigenvectors)\n",
    "# eigenvalues (already sorted highest to lowest)\n",
    "eigenvalues_sum = sum(sklearn_pca.explained_variance_)\n",
    "explained_variance = sklearn_pca.explained_variance_/eigenvalues_sum\n",
    "print(\"Explained Variance:\\n%s\\n\" %explained_variance)\n",
    "\n",
    "# explained variance shows we really only need the eigenvectors associated with the first two eigenvalues\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# apply the feature vector to the data set\n",
    "pca_data_set = pd.DataFrame(sklearn_pca.transform(data_set_std))\n",
    "print(\"Dimensionally-reduced data set (tail):\")\n",
    "display(pca_data_set.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA On Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use PCA on the wdbc data set in order to determine which variables explain most of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix shape:\n",
      "(30, 30)\n",
      "\n",
      "Eigenvectors shape:\n",
      "(30, 30)\n",
      "\n",
      "Explained Variance:\n",
      "[  4.42720256e-01   1.89711820e-01   9.39316326e-02   6.60213492e-02\n",
      "   5.49576849e-02   4.02452204e-02   2.25073371e-02   1.58872380e-02\n",
      "   1.38964937e-02   1.16897819e-02   9.79718988e-03   8.70537901e-03\n",
      "   8.04524987e-03   5.23365745e-03   3.13783217e-03   2.66209337e-03\n",
      "   1.97996793e-03   1.75395945e-03   1.64925306e-03   1.03864675e-03\n",
      "   9.99096464e-04   9.14646751e-04   8.11361259e-04   6.01833567e-04\n",
      "   5.16042379e-04   2.72587995e-04   2.30015463e-04   5.29779290e-05\n",
      "   2.49601032e-05   4.43482743e-06]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data set\n",
    "wdbc_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "data_set = pd.io.parsers.read_csv(wdbc_url, header=None)\n",
    "data_set = data_set.iloc[:,2:]\n",
    "data_set_std = StandardScaler().fit_transform(data_set)\n",
    "sklearn_pca = sklearnPCA(n_components=data_set.shape[1])\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# covariance matrix\n",
    "covariance_matrix = sklearn_pca.get_covariance()\n",
    "print(\"Covariance matrix shape:\\n{0}\\n\".format(covariance_matrix.shape))\n",
    "# eigenvectors\n",
    "eigenvectors = sklearn_pca.components_.T\n",
    "print(\"Eigenvectors shape:\\n{0}\\n\".format(eigenvectors.shape))\n",
    "# eigenvalues\n",
    "eigenvalues_sum = sum(sklearn_pca.explained_variance_)\n",
    "explained_variance = sklearn_pca.explained_variance_/eigenvalues_sum\n",
    "print(\"Explained Variance:\\n%s\\n\" %explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 eigenvalues explain 95% of the variance, so let's just keep the first 10 components (i.e. eigenvectors associated with those eigenvalues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951568814337\n"
     ]
    }
   ],
   "source": [
    "print(sum(explained_variance[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375193</td>\n",
       "      <td>-0.596130</td>\n",
       "      <td>-0.035470</td>\n",
       "      <td>0.987928</td>\n",
       "      <td>0.256988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.246710</td>\n",
       "      <td>-0.716326</td>\n",
       "      <td>-1.113359</td>\n",
       "      <td>-0.105208</td>\n",
       "      <td>-0.108633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192758</td>\n",
       "      <td>0.341887</td>\n",
       "      <td>0.393917</td>\n",
       "      <td>0.520877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567936</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>-0.280239</td>\n",
       "      <td>-0.542034</td>\n",
       "      <td>-0.089296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617837</td>\n",
       "      <td>1.698952</td>\n",
       "      <td>1.046353</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>-0.047726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "564   6.439315 -3.576817  2.459487  1.177314 -0.074824 -2.375193 -0.596130   \n",
       "565   3.793382 -3.584048  2.088476 -2.506028 -0.510723 -0.246710 -0.716326   \n",
       "566   1.256179 -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192758   \n",
       "567  10.374794  1.672010 -1.877029 -2.356031 -0.033742  0.567936  0.223082   \n",
       "568  -5.475243 -0.670637  1.490443 -2.299157 -0.184703  1.617837  1.698952   \n",
       "\n",
       "            7         8         9  \n",
       "564 -0.035470  0.987928  0.256988  \n",
       "565 -1.113359 -0.105208 -0.108633  \n",
       "566  0.341887  0.393917  0.520877  \n",
       "567 -0.280239 -0.542034 -0.089296  \n",
       "568  1.046353  0.374101 -0.047726  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_pca = sklearnPCA(n_components=10)\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# apply the feature vector to the data set\n",
    "pca_data_set = pd.DataFrame(sklearn_pca.transform(data_set_std))\n",
    "# display the tail of the reduced data set\n",
    "display(pca_data_set.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [plotly PCA tutorial](https://plot.ly/ipython-notebooks/principal-component-analysis/#Standardizing)\n",
    "* [otago PCA tutorial](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "* [wdbc data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With the `food-texture.csv`, build a PCA model with 5 components,1 then detrmine the variable that should be kept, with a 95% tolerance.\n",
    "2. Using `room-temperature.csv`, generate a PCA model with 4 components, and provide the covariance matrix and the explained variance. Based on the explained variance, what data columns should we keep if our tolerance is 90%? Refit the model with the components we need.\n",
    "3. With the `silicon-wafer-thickness.csv`, build a PCA model using all the components of the data, and determine the number of components for a data set with 90% tolerance of variance and 95% tolerance. Refit the model for 90% and 95%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
